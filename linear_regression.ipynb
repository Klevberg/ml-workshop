{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9c6241",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b25da1-a18c-49f0-9539-20eafa8861e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16716b15",
   "metadata": {},
   "source": [
    "## Functions for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d8a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_data(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Plots training and validation data side by side\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels \n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Plot training data\n",
    "    ax1.set_title(\"Training Data\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.scatter(X_train, y_train, color=\"green\")\n",
    "\n",
    "    # Plot validation data \n",
    "    ax2.set_title(\"Validation Data\")\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"y\")\n",
    "    ax2.scatter(X_val, y_val, color=\"orange\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_linear_regression(X: np.ndarray, y: np.ndarray, weights: np.ndarray, title: str = \"Data points and fitted linear function\", scatter_color: str = \"green\"):\n",
    "    \"\"\"\n",
    "    Plots the data points as a scatter plot and the fitted linear regression line\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: Target values \n",
    "        weights: Regression weights [intercept, slope] where y = slope*x + intercept\n",
    "        title: Plot title\n",
    "        scatter_color: Color of the scatter plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.scatter(X, y, alpha=0.5, color=scatter_color)\n",
    "    x_values = np.array(plt.gca().get_xlim())\n",
    "    ax.plot(x_values, weights[0] + weights[1]*x_values, color=\"red\")\n",
    "\n",
    "\n",
    "def plot_validation_results(learning_rates, validation_results):\n",
    "    \"\"\"\n",
    "    Plots the validation MSE vs iterations for different learning rates.\n",
    "    \n",
    "    Args:\n",
    "        learning_rates: List of learning rates used in the experiment.\n",
    "        validation_results: Dictionary containing 'lr', 'iterations', and 'mse' lists.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    for lr_val in learning_rates:\n",
    "        mask = np.array(validation_results['lr']) == lr_val\n",
    "        ax.plot(np.array(validation_results['iterations'])[mask],\n",
    "                np.array(validation_results['mse'])[mask],\n",
    "                'o-', label=f'lr={lr_val:.5f}')\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Validation MSE')\n",
    "    ax.set_title('Validation MSE vs Number of Iterations')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_regression_progression(X_train, y_train, X_val, y_val, gradient_descent, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Plot validation data and regression lines showing progression of gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels \n",
    "        X_val: Validation features\n",
    "        y_val: Validation labels\n",
    "        gradient_descent: Gradient descent function\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot validation data points\n",
    "    ax.scatter(X_val, y_val, color='orange', alpha=0.75, label='Validation Data')\n",
    "\n",
    "    x_line = np.array([min(X_val), max(X_val)])\n",
    "\n",
    "    # Plot regression lines at different iterations with increasing opacity\n",
    "    iterations_to_show = [0, 5, 10, 25, 100, 500, 1000, 1500]\n",
    "\n",
    "    for i, iteration in enumerate(iterations_to_show):\n",
    "        # Get weights for this iteration\n",
    "        weights = gradient_descent(X_train, y_train, learning_rate=learning_rate, iterations=iteration)\n",
    "        \n",
    "        # Generate points for the line\n",
    "        y_line = weights[0] + weights[1] * x_line\n",
    "        \n",
    "        # Plot line with increasing opacity\n",
    "        alpha = 0.1 + (i * 0.9 / len(iterations_to_show))\n",
    "        if iteration == 0:\n",
    "            ax.plot(x_line, y_line, '-', color='green', alpha=0.5, label=f'Iteration {iteration}')\n",
    "        else:\n",
    "            ax.plot(x_line, y_line, '-', color='black', alpha=alpha, label=f'Iteration {iteration}')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Validation Data with Regression Line Progression\\n0 to 1500 iterations, learning rate = {learning_rate}')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_surface(X, y, calculate_loss_and_gradient, dim='2d', lr=0.05, n_iter=100, init_weights=[0, 0]):\n",
    "    \"\"\"\n",
    "    Plots MSE loss function as contour (2d) or surface (3d) plot.\n",
    "    Args:\n",
    "        X, y: Data points\n",
    "        gradient_loss_function, mse_loss: Functions to compute the gradient and loss\n",
    "        dim: '2d' or '3d'\n",
    "        lr: Learning rate\n",
    "        n_iter: Number of iterations\n",
    "        init_weights: Starting weights\n",
    "    \"\"\"\n",
    "    if dim not in ['2d', '3d']:\n",
    "        raise ValueError(\"dim must be '2d' or '3d'\")\n",
    "    \n",
    "    # Create loss surface\n",
    "    W0, W1 = np.meshgrid(np.linspace(-200, 200, 400), np.linspace(-100, 130, 230))\n",
    "    loss = np.array([[calculate_loss_and_gradient(X, y, [w0, w1])[0] for w0, w1 in zip(row0, row1)] \n",
    "                     for row0, row1 in zip(W0, W1)])\n",
    "    \n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax = ax if dim == '2d' else fig.add_subplot(111, projection='3d')\n",
    "    ax.set_title(f\"MSE Loss {'Contour' if dim == '2d' else 'Surface'}\")\n",
    "    ax.set_xlabel(\"Intercept\"), ax.set_ylabel(\"Slope\")\n",
    "    \n",
    "    # Plot loss surface\n",
    "    if dim == '2d':\n",
    "        surf = ax.contourf(W0, W1, loss, levels=500, cmap=\"coolwarm\", alpha=0.8)\n",
    "    else:\n",
    "        surf = ax.plot_surface(W0, W1, loss, cmap='coolwarm', alpha=0.8)\n",
    "        ax.set_zlabel(\"Loss\")\n",
    "    fig.colorbar(surf)\n",
    "    \n",
    "    # Plot gradient descent path\n",
    "    weights = np.array(init_weights, dtype=float)\n",
    "    path = [weights.copy()]\n",
    "    for _ in range(n_iter):\n",
    "        weights -= lr * calculate_loss_and_gradient(X, y, weights)[1]\n",
    "        path.append(weights.copy())\n",
    "    path = np.array(path)\n",
    "    \n",
    "    if dim == '2d':\n",
    "        ax.plot(path[:, 0], path[:, 1], 'k.', markersize=1)\n",
    "    else:\n",
    "        ax.plot(path[:, 0], path[:, 1], [calculate_loss_and_gradient(X, y, w)[0] for w in path], \n",
    "               'k.', label='Gradient descent path')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362137e",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Generate a linear regression dataset\n",
    "X, y = datasets.make_regression(\n",
    "    n_samples=100, \n",
    "    n_features=1, \n",
    "    noise=10,\n",
    "    random_state=SEED,\n",
    "    bias=100.0\n",
    ")\n",
    "X = X.flatten() * 5 # Makes this example more interesting\n",
    "\n",
    "# Display first few samples of the dataset\n",
    "print(\"First 5 samples of the dataset:\")\n",
    "for i in range(5):\n",
    "    print(f\"X: {X[i]:.2f}, y: {y[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130785c",
   "metadata": {},
   "source": [
    "## Split the data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c02848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets (60%, 20%, 20%)\n",
    "X_train, X_temp, y_train, y_temp = None, None, None, None   # TODO\n",
    "X_val, X_test, y_val, y_test = None, None, None, None       # TODO\n",
    "\n",
    "# Scale the data using mean and standard deviation and using only training data statistics\n",
    "# X_mean = np.mean(X_train)\n",
    "# X_std = np.std(X_train)\n",
    "# X_train = (X_train - X_mean) / X_std\n",
    "# X_val = (X_val - X_mean) / X_std\n",
    "# X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# y_mean = np.mean(y_train)\n",
    "# y_std = np.std(y_train)\n",
    "# y_train = (y_train - y_mean) / y_std\n",
    "# y_val = (y_val - y_mean) / y_std\n",
    "# y_test = (y_test - y_mean) / y_std\n",
    "\n",
    "\n",
    "plot_train_val_data(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a226982",
   "metadata": {},
   "source": [
    "## Functions for linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58599391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute predictions using linear regression\"\"\"\n",
    "    \n",
    "    ... # TODO\n",
    "\n",
    "\n",
    "def compute_loss_and_gradient(X: np.ndarray, y: np.ndarray, weights: np.ndarray) -> tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes both MSE loss and its gradient in a single pass through the data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (loss, gradient)\n",
    "    \"\"\"\n",
    "\n",
    "    ... # TODO\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray,\n",
    "    weights: np.ndarray=np.array([0, 0]),\n",
    "    learning_rate: float=0.1,\n",
    "    iterations: int=1000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Iteratively updates the weights using the steepest descent method\n",
    "    \"\"\"\n",
    "    \n",
    "    ... # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d5743",
   "metadata": {},
   "source": [
    "## Run gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53334e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = None # TODO\n",
    "\n",
    "print(f\"Found weights: {weights}\")\n",
    "\n",
    "plot_linear_regression(X_train, y_train, weights, title=\"Linear Regression - Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_regression(X_val, y_val, weights, title=\"Linear Regression - Validation Data\", scatter_color=\"orange\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58cb55",
   "metadata": {},
   "source": [
    "## Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [] # TODO\n",
    "iterations = []     # TODO\n",
    "\n",
    "best_learning_rate = None\n",
    "best_iterations = None\n",
    "best_weights = None\n",
    "best_mse_val_loss = float('inf')\n",
    "\n",
    "plt.validation_results = {'lr': [], 'iterations': [], 'mse': []}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for iteration in iterations:\n",
    "        weights = None                  # TODO\n",
    "        mse_val_loss, _ = None, None    # TODO\n",
    "        \n",
    "        # Store results for plotting\n",
    "        plt.validation_results['lr'].append(learning_rate)\n",
    "        plt.validation_results['iterations'].append(iteration)\n",
    "        plt.validation_results['mse'].append(mse_val_loss)\n",
    "        \n",
    "        print(f\"Learning rate = {learning_rate:.5f} | Iterations = {iteration}\\tMSE on validation set: {mse_val_loss}\")\n",
    "        \n",
    "        # Update best hyperparameters if current validation loss is lower\n",
    "        ... # TODO\n",
    "\n",
    "\n",
    "plot_validation_results(learning_rates, plt.validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b166952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest learning rate: {best_learning_rate:.5f}\")\n",
    "print(f\"Best iterations: {best_iterations}\")\n",
    "print(f\"Best MSE on validation set: {best_mse_val_loss}\")\n",
    "\n",
    "plot_linear_regression(X_val, y_val, best_weights, title=\"Linear Regression - Validation Data\", scatter_color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff651f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_progression(X_train, y_train, X_val, y_val, gradient_descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ba885",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation sets\n",
    "X_combined = None # TODO\n",
    "y_combined = None # TODO\n",
    "\n",
    "# Train the model on combined data with best hyperparameters\n",
    "weights = None # TODO\n",
    "\n",
    "# Evaluate on test set\n",
    "mse_test_loss, _ = None, None # TODO\n",
    "print(f\"MSE on test set: {mse_test_loss}\")\n",
    "\n",
    "\n",
    "plot_linear_regression(X_test, y_test, weights, title=\"Linear Regression - Test Data\", scatter_color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033640df",
   "metadata": {},
   "source": [
    "## Visualize gradient descent on loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30865f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(X, y, compute_loss_and_gradient, dim='2d', lr=best_learning_rate, n_iter=best_iterations, init_weights=[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a484b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(X, y, compute_loss_and_gradient, dim='3d', lr=best_learning_rate, n_iter=best_iterations, init_weights=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fceb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(X, y, compute_loss_and_gradient, dim='3d', lr=0.005, n_iter=500, init_weights=[-150, 110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4353dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_surface(X, y, compute_loss_and_gradient, dim='3d', lr=0.05, n_iter=100, init_weights=[-150, 110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
